{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4",
   "authorship_tag": "ABX9TyNm+HSglLWqmXuZIs5XsRo4",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eglantinc/image-classification/blob/main/feature_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_r4Lq17VeEmg"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input as preprocess_vgg\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input as preprocess_inception\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input as preprocess_resnet\n",
    "from pathlib import Path\n",
    "import kagglehub\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def choose_model(chosen_model):\n",
    "    \"\"\"\n",
    "    Chooses the pre-trained Keras model based on the provided name.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to use ('vgg16', 'alexnet', 'inceptionv3').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Keras model, target image size, and preprocessing function.\n",
    "    Raises:\n",
    "        ValueError: If the model name is not supported.\n",
    "    \"\"\"\n",
    "    if chosen_model.lower() == \"alexnet\":\n",
    "        print(\"Using AlexNet model...\")\n",
    "        model = ResNet50(weights='imagenet', include_top=False, pooling='avg')\n",
    "        target_size = (224, 224)\n",
    "        preprocess_fn = preprocess_resnet\n",
    "    elif chosen_model.lower() == \"vgg16\":\n",
    "        print(\"Using VGG16 model...\")\n",
    "        model = VGG16(weights='imagenet', include_top=False, pooling='avg')\n",
    "        target_size = (224, 224)\n",
    "        preprocess_fn = preprocess_vgg\n",
    "    elif chosen_model.lower() == \"inceptionv3\":\n",
    "        print(\"Using InceptionV3 model...\")\n",
    "        model = InceptionV3(weights='imagenet', include_top=False, pooling='avg')\n",
    "        target_size = (299, 299)\n",
    "        preprocess_fn = preprocess_inception\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model. Choose from 'vgg16', 'alexnet', or 'inceptionv3'.\")\n",
    "    return model, target_size, preprocess_fn"
   ],
   "metadata": {
    "id": "WHUmUte8eInv"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def extract_features(image_path, target_size, model, preprocess_fn):\n",
    "    \"\"\"\n",
    "    Charges the image from 'image_path', resizes it to 'target_size',\n",
    "    converts it to a NumPy array, applies preprocessing, and extracts\n",
    "    the feature vector using the provided model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        target_size(tuple) : Target size (height, width) to resize the image.\n",
    "        model (keras.Model): Pre-trained model to extract features.\n",
    "        preprocess_fn (function): Preprocessing function specific to the model.\n",
    "    Returns:\n",
    "        numpy.ndarray: Extracted feature vector (flattened to 1D).\n",
    "    \"\"\"\n",
    "    img = load_img(image_path, target_size=target_size)\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_fn(img_array)\n",
    "    features = model.predict(img_array)\n",
    "    return features.flatten()"
   ],
   "metadata": {
    "id": "e7d26gg5eRGO"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def create_vector(image_path, target_size, model, preprocess_fn):\n",
    "    \"\"\"\n",
    "    Creates a feature vector from an image using the specified model.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image.\n",
    "        target_size (tuple): Target size (height, width) to resize the image.\n",
    "        model (keras.Model): Pre-trained model to extract features.\n",
    "        preprocess_fn (function): Preprocessing function specific to the model.\n",
    "\n",
    "    Returns:\n",
    "        list: feature vector with the label (folder name) appended at the end.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    features = extract_features(image_path, target_size, model, preprocess_fn)\n",
    "    return features.tolist() + [Path(image_path).parts[-2]]"
   ],
   "metadata": {
    "id": "X7FPRR7feUmy"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    all_models = [ \"alexnet\", \"inceptionv3\", \"vgg16\"]\n",
    "\n",
    "    # Create the Dataset directory if it doesn't exist\n",
    "    dataset_dir = \"Dataset\"\n",
    "    os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    for model_name in all_models:\n",
    "        print(f\"Processing with model: {model_name}\")\n",
    "        model, target_size, preprocess_fn = choose_model(model_name)\n",
    "        print(\"-\" * 1000)\n",
    "        # Download latest version\n",
    "        parent_folder = kagglehub.dataset_download(\"amerzishminha/forest-fire-smoke-and-non-fire-image-dataset\")\n",
    "\n",
    "        subfolder_name = \"train\"\n",
    "        subfolder_path = next(Path(parent_folder).rglob(subfolder_name))\n",
    "        print(subfolder_path)\n",
    "\n",
    "        wildfire_path = [subfolder_path / \"fire\", subfolder_path / \"non fire\", subfolder_path / \"Smoke\"]\n",
    "\n",
    "        features = []\n",
    "        for folder in wildfire_path:\n",
    "            number_of_images = 0\n",
    "            print(\"Processing folder:\", folder)\n",
    "            for image_file in folder.iterdir():\n",
    "                number_of_images += 1\n",
    "                print(f\"Processing of image: {image_file.name}\")\n",
    "                features_vector = create_vector(str(image_file), target_size, model, preprocess_fn)\n",
    "                print(f\"Feature vector for {image_file.name}: {features_vector[:5]} ... {features_vector[-5:]}\")\n",
    "                features.append(features_vector)\n",
    "                print(\"-\" * 1000)\n",
    "\n",
    "                csv_file_path = f\"{dataset_dir}/{model_name}.csv\"\n",
    "                print(f\"Writing features to {csv_file_path}...\")\n",
    "                with open(csv_file_path, mode='w', newline='') as csv_file:\n",
    "                    writer = csv.writer(csv_file)\n",
    "                    # Create header based on feature length minus label column and add 'label' at the end\n",
    "                    header = [f\"feature_{i + 1}\" for i in range(len(features[0]) - 1)] + [\"label\"]\n",
    "                    writer.writerow(header)\n",
    "                    writer.writerows(features)\n",
    "\n",
    "                print(f\"Features written to {csv_file_path}\")\n",
    "\n",
    "                # Taking just the first 200 images of the folder to create the csv files\n",
    "                if number_of_images >= 200:\n",
    "                  print(\"Reached the limit of 200 images. Exiting the loop.\")\n",
    "                  break\n"
   ],
   "metadata": {
    "id": "B1aIXXx9eZrG"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-D9IRct1es57",
    "outputId": "23e12f50-d829-4986-8560-4a1b8f2b4153"
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
